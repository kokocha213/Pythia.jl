var documenterSearchIndex = {"docs":
[{"location":"examples/arma_sunspots/#ARMA-Modeling-and-Residual-Diagnostics-in-Julia","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"ARMA Modeling and Residual Diagnostics in Julia","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"It explores AR models on the sunspots dataset, generates a simulated ARMA(4,1) process,  and examines residual diagnostics.","category":"page"},{"location":"examples/arma_sunspots/#example-block:-loads-all-necessary-packages-and-fixes-the-seed","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"example block: loads all necessary packages and fixes the seed","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"using Pythia\nusing Plots\nusing Random\nusing LinearAlgebra\n\nRandom.seed!(1234) ","category":"page"},{"location":"examples/arma_sunspots/#Load-and-visualize-sunspots-data","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Load and visualize sunspots data","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"sun_data = load_dataset(\"sunspots\")\nyears = round.(Int, sun_data.Year)\nssn   = sun_data.SSN","category":"page"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"plot(\n    years,\n    ssn,\n    xlabel = \"Year\",\n    ylabel = \"Sunspot Number (SSN)\",\n    title = \"Yearly Sunspot Activity\",\n    lw = 2,\n    legend = false,\n    size = (1200, 800)\n)","category":"page"},{"location":"examples/arma_sunspots/#ACF-and-PACF","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"ACF and PACF","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"plot_acf_pacf(ssn)","category":"page"},{"location":"examples/arma_sunspots/#Fit-AR(2)-model","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Fit AR(2) model","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"model_ar2 = Pythia.ARIMAModel(ssn; p=2, q=0, seasonal=false, stationary=true, approximation=false)\nfitted_ar2 = fit(model_ar2)\nget_ic(fitted_ar2)\nplt, pval = check_residuals(fitted_ar2)\nplt","category":"page"},{"location":"examples/arma_sunspots/#Fit-AR(3)-model","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Fit AR(3) model","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"model_ar3 = Pythia.ARIMAModel(ssn; p=3, q=0, seasonal=false, stationary=true, approximation=false)\nfitted_ar3 = fit(model_ar3)\nget_ic(fitted_ar3)\nplt, pval = check_residuals(fitted_ar3)\nplt","category":"page"},{"location":"examples/arma_sunspots/#Function-to-generate-ARMA-process","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Function to generate ARMA process","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"function generate_arma(arparams::Vector{Float64}, maparams::Vector{Float64}, n::Int; burnin=250)\n    p = length(arparams) - 1\n    q = length(maparams) - 1\n\n    φ = -arparams[2:end]  # statsmodels convention\n    θ =  maparams[2:end]\n\n    N = n + burnin\n    y = zeros(Float64, N)\n    ε = randn(N)\n\n    for t in 1:max(p, q, N)\n        if p > 0 && t > p\n            y[t] += dot(φ, y[t-1:-1:t-p])\n        end\n        if q > 0 && t > q\n            y[t] += dot(θ, ε[t-1:-1:t-q])\n        end\n        y[t] += ε[t]\n    end\n\n    return y[burnin+1:end]\nend","category":"page"},{"location":"examples/arma_sunspots/#Simulate-ARMA(4,1)","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Simulate ARMA(4,1)","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"arparams = [1.0, 0.35, -0.15, 0.55, 0.1]\nmaparams = [1.0, 0.65]\nsim_data = generate_arma(arparams, maparams, 500)","category":"page"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"plot_acf_pacf(sim_data)","category":"page"},{"location":"examples/arma_sunspots/#Fit-ARMA(1,1)-(incorrect)-on-simulated-data","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Fit ARMA(1,1) (incorrect) on simulated data","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"model_arma11 = Pythia.ARIMAModel(sim_data; p=1, q=1, seasonal=false, stationary=true, approximation=false)\nfitted_arma11 = fit(model_arma11)\nplt, pval = check_residuals(fitted_arma11)\nplt","category":"page"},{"location":"examples/arma_sunspots/#Fit-ARMA(4,1)-(correct)-on-simulated-data","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Fit ARMA(4,1) (correct) on simulated data","text":"","category":"section"},{"location":"examples/arma_sunspots/","page":"Autoregressive Moving Average (ARMA): Sunspots data","title":"Autoregressive Moving Average (ARMA): Sunspots data","text":"model_arma41 = Pythia.ARIMAModel(sim_data; p=4, q=1, seasonal=false, stationary=true, approximation=false)\nfitted_arma41 = fit(model_arma41)\nplt, pval = check_residuals(fitted_arma41)\nplt","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The following are the public functions, types, and methods exported by Pythia.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [Pythia]","category":"page"},{"location":"api/#Pythia.ARIMAModel","page":"API Reference","title":"Pythia.ARIMAModel","text":"ARIMAModel\n\nA comprehensive implementation of AutoRegressive Integrated Moving Average (ARIMA) models  with seasonal extensions (SARIMA), based on the Hyndman-Khandakar algorithm.\n\nThis implementation supports both automatic and manual model specification, with various  optimization strategies and information criteria for model selection.\n\nMathematical Background\n\nARIMA(p,d,q) × (P,D,Q)s models are defined by:\n\nφ(B)(1-B)^d Φ(B^s)(1-B^s)^D yt = θ(B)Θ(B^s)εt + μ\n\nWhere:\n\nφ(B) = 1 - φ₁B - φ₂B² - ... - φₚBᵖ (non-seasonal AR polynomial)\nΦ(B^s) = 1 - Φ₁B^s - Φ₂B^{2s} - ... - ΦₚB^{Ps} (seasonal AR polynomial)  \nθ(B) = 1 + θ₁B + θ₂B² + ... + θₑB^q (non-seasonal MA polynomial)\nΘ(B^s) = 1 + Θ₁B^s + Θ₂B^{2s} + ... + ΘₖB^{Qs} (seasonal MA polynomial)\nB is the backshift operator\nε_t ~ N(0,σ²) are white noise errors\n\nReferences\n\nHyndman, R.J., & Khandakar, Y. (2008). \"Automatic time series forecasting: the forecast package for R\". Journal of Statistical Software, 26(3), 1-22.\nBox, G.E.P., Jenkins, G.M., Reinsel, G.C., & Ljung, G.M. (2015). \"Time Series Analysis: Forecasting and Control\". 5th Edition.\n\n\n\n\n\n","category":"type"},{"location":"api/#Pythia.ARIMAModel-Tuple{AbstractVector{<:Real}}","page":"API Reference","title":"Pythia.ARIMAModel","text":"ARIMAModel(y; kwargs...)\n\nCreate an ARIMA model with automatic parameter selection capabilities.\n\nThis constructor implements the Hyndman-Khandakar algorithm for automatic ARIMA model  selection, which combines unit root tests, information criteria, and stepwise search  to find optimal model parameters.\n\nArguments\n\ny::AbstractVector{<:Real}: Time series data (required)\n\nKeyword Arguments\n\nh::Int=5: Forecast horizon (number of periods to forecast)\np::Union{Nothing,Int}=nothing: Non-seasonal AR order (auto-determined if nothing)\nd::Union{Nothing,Int}=nothing: Non-seasonal differencing order (auto-determined if nothing)\nq::Union{Nothing,Int}=nothing: Non-seasonal MA order (auto-determined if nothing)\nP::Union{Nothing,Int}=nothing: Seasonal AR order (auto-determined if nothing)\nD::Union{Nothing,Int}=nothing: Seasonal differencing order (auto-determined if nothing)\nQ::Union{Nothing,Int}=nothing: Seasonal MA order (auto-determined if nothing)\ns::Union{Nothing,Int}=nothing: Seasonal period (e.g., 12 for monthly data)\nseasonal::Bool=true: Enable seasonal modeling\nic::String=\"aicc\": Information criterion (\"aic\", \"aicc\", \"bic\")\nloss::Function=sse_arima_loss_: Loss function for parameter estimation\noptimizer::Function=() -> Optim.LBFGS(): Optimization algorithm\nstationary::Bool=false: Assume series is already stationary\ntrace::Bool=false: Print optimization progress\nstepwise::Bool=true: Use stepwise search (recommended for speed)\napproximation::Bool=true: Use CSS approximation for speed\nmax_p::Int=5: Maximum non-seasonal AR order to consider\nmax_q::Int=5: Maximum non-seasonal MA order to consider\nmax_P::Int=2: Maximum seasonal AR order to consider\nmax_Q::Int=2: Maximum seasonal MA order to consider\nmax_d::Int=2: Maximum non-seasonal differencing order\nmax_D::Int=1: Maximum seasonal differencing order\nauto::Bool=true: Enable automatic model selection\n\nExamples\n\n# Basic automatic ARIMA for quarterly data\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nmodel = ARIMAModel(y; s=4)  # s=4 for quarterly seasonality\n\n# Manual ARIMA(1,1,1) specification\nmodel = ARIMAModel(y; p=1, d=1, q=1, auto=false)\n\n# High-frequency data with custom settings\ndaily_data = rand(365) .+ sin.(2π * (1:365) / 365)  # Daily data with annual cycle\nmodel = ARIMAModel(daily_data; \n                   s=365, \n                   h=30,           # 30-day forecast\n                   max_p=3,        # Limit AR terms\n                   max_q=3,        # Limit MA terms\n                   approximation=false,  # Use exact ML\n                   ic=\"bic\")       # Use BIC for selection\n\n# Non-seasonal ARIMA with custom optimizer\nmodel = ARIMAModel(y; \n                   seasonal=false, \n                   optimizer=() -> Optim.NelderMead(),\n                   trace=true)\n\nMathematical Details\n\nThe constructor implements the Hyndman-Khandakar algorithm:\n\nDetermine differencing orders (d, D) using unit root tests\nSelect initial parameter values from a restricted set\nUse stepwise search to explore neighboring parameter combinations\nSelect final model based on information criterion\n\nReferences\n\nHyndman, R.J., & Khandakar, Y. (2008). \"Automatic time series forecasting: the forecast package for R\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.SES","page":"API Reference","title":"Pythia.SES","text":"SES Description:     - Contains parameters and data to apply Simple Exponential Smoothing (SES)     - Subtype of ETSModel, for future proofing Constuctor:     - Vector{AbstractFloat} y: contains observations for forecasting     - Integer h: Stores the number of steps after time T to forecast (T = latest time in the observed data)         - Default = 10, must be > 0     - Union{Nothing, Float64} alpha: smoothing parameter [1]         - 0 < alpha < 1         - If no value is specified, an optimal value will be chosen using LBFGS.     - Union{Nothing, Float64} initlevel: initial level value [1]         -Inf < initlevel < Inf         - If no value is specified, an optimal value will be chosen uisng LBFGS.\n\nEXAMPLE USAGE:     observations = [1.0, 2.0, 3.0]     # Inputted values will be used if they are specified. Otherwise, they will be computed.     mdl = SES(observations, h = 5)      mdl = SES(observations, alpha = 0.4)      mdl = SES(observations, alpha = 0.25, initlevel = 500.0)      mdl = SES(observations, h = 15, alpha = 0.3, initlevel = 750.0)\n\n\n\n\n\n","category":"type"},{"location":"api/#Pythia.SSE_-Tuple{SES}","page":"API Reference","title":"Pythia.SSE_","text":"SSE()  Description:     - Returns the Sum of Squared Errors for a given alpha and initlevel. Parameters:     - model: SES struct (defined above)     - alpha: value of alpha used for computation     - initlevel: value of initlevel used for computation     - verbose: if verbose > 0, then the parameters will be printed as the are optimized. Returns:     - Float64 res: the Sum of Squared Errors for the given alpha and init_level values.\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.check_residuals-Tuple{Any}","page":"API Reference","title":"Pythia.check_residuals","text":"check_residuals(model; lag=24, plot_diagnostics=true)\n\nDiagnostic checks for ARIMA residuals.\n\nArguments\n\nmodel: a fitted ARIMA model with residuals stored in model.fitted_model.residuals.\n\nKeyword Arguments\n\nlag::Int = 24: maximum lag for the ACF and Ljung-Box test.\nplot_diagnostics::Bool = true: whether to generate diagnostic plots.\n\nReturns\n\nplt::Plots.Plot (or nothing if plot_diagnostics=false): the combined residual diagnostic plot.\npval::Float64: Ljung–Box p-value for the residuals.\n\nThis function does the following:\n\nPlots residual diagnostics (if plot_diagnostics=true):\nResiduals vs time\nACF of residuals with 95% confidence bands\nHistogram of residuals with fitted Normal distribution overlay\nComputes the Ljung–Box test for white noise in residuals.\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.cleanParams_","page":"API Reference","title":"Pythia.cleanParams_","text":"cleanParams_() Description:     Throws an error if a given parameters is out of the specified bounds. Parameters:     - param: value of parameter to be checked     - name: name of parameter to be checked     - lb: lower bound of parameter, default = 0.0     - ub: upper bound of parameter, defualt = 1.0 Note: if param is not of type Nothing, then param must be in (0.0, 1.0) Returns:     - nothing\n\n\n\n\n\n","category":"function"},{"location":"api/#Pythia.css_arima_loss_-Tuple{AbstractVector{<:Real}, Vector{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64, Int64}","page":"API Reference","title":"Pythia.css_arima_loss_","text":"css_arima_loss_(y, φ, θq, Φ, Θ, μ, s; return_residuals=false) -> Float64 or Vector{Float64}\n\nCalculate the Conditional Sum of Squares (CSS) loss for ARIMA model parameters.\n\nThis is a fast approximation to maximum likelihood estimation that conditions on  initial values and computes the sum of squared residuals. It's particularly useful  for initial parameter estimation and model comparison.\n\nArguments\n\ny::AbstractVector{<:Real}: Time series data\nφ::Vector{Float64}: Non-seasonal AR coefficients [φ₁, φ₂, ..., φₚ]\nθq::Vector{Float64}: Non-seasonal MA coefficients [θ₁, θ₂, ..., θₑ]\nΦ::Vector{Float64}: Seasonal AR coefficients [Φ₁, Φ₂, ..., Φₚ]\nΘ::Vector{Float64}: Seasonal MA coefficients [Θ₁, Θ₂, ..., Θₖ]\nμ::Float64: Intercept/mean parameter\ns::Int: Seasonal period\n\nKeyword Arguments\n\nreturn_residuals::Bool=false: Return residuals instead of sum of squares\n\nReturns\n\nFloat64: Sum of squared residuals (if return_residuals=false)\nVector{Float64}: Residual series (if return_residuals=true)\n\nExamples\n\n# Example for ARIMA(1,1,1)×(1,1,1)₁₂\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nφ = [0.5]        # AR(1) coefficient\nθq = [0.3]       # MA(1) coefficient  \nΦ = [0.2]        # Seasonal AR(1) coefficient\nΘ = [0.1]        # Seasonal MA(1) coefficient\nμ = 0.0          # Mean (usually 0 for differenced data)\ns = 12           # Monthly seasonality\n\n# Calculate loss\nloss = css_arima_loss_(y, φ, θq, Φ, Θ, μ, s)\nprintln(\"CSS Loss: \", loss)\n\n# Get residuals\nresiduals = css_arima_loss_(y, φ, θq, Φ, Θ, μ, s; return_residuals=true)\n\nMathematical Formula\n\nThe CSS approximation computes residuals as: eₜ = yₜ - μ - Σᵢ₌₁ᵖ φᵢyₜ₋ᵢ - Σᵢ₌₁ᵖ Φᵢyₜ₋ᵢₛ - Σⱼ₌₁ᵈ θⱼeₜ₋ⱼ - Σⱼ₌₁ᵈ Θⱼeₜ₋ⱼₛ\n\nLoss = Σₜ₌ₘ₊₁ⁿ eₜ²\n\nWhere m = max(p, q, P×s, Q×s) is the maximum lag.\n\nReferences\n\nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (2015). \"Time Series Analysis: Forecasting and Control\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.difference_series_-Tuple{AbstractVector{<:Real}}","page":"API Reference","title":"Pythia.difference_series_","text":"difference_series_(y; kwargs...) -> NamedTuple\n\nApply non-seasonal and seasonal differencing to achieve stationarity.\n\nThis function implements the differencing component of the Hyndman-Khandakar algorithm, using unit root tests to determine appropriate differencing orders.\n\nArguments\n\ny::AbstractVector{<:Real}: Time series data\n\nKeyword Arguments\n\nd::Union{Nothing,Int}=nothing: Non-seasonal differencing order (auto-determined if nothing)\nD::Union{Nothing,Int}=nothing: Seasonal differencing order (auto-determined if nothing)\ns::Union{Nothing,Int}=nothing: Seasonal period\nalpha::Float64=0.05: Significance level for unit root tests\nmax_d::Int=2: Maximum non-seasonal differencing order\nmax_D::Int=2: Maximum seasonal differencing order\ntest::String=:kpss: Unit root test for non-seasonal differencing\nseasonal_test::String=:ocsb: Unit root test for seasonal differencing\ntrend::Bool=true: Include trend in unit root tests\n\nReturns\n\nNamedTuple: Contains y_diff (final differenced series), d (applied non-seasonal order),  D (applied seasonal order), y_diff_naive (after non-seasonal differencing only)\n\nExamples\n\n# Automatic differencing for monthly data\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nresult = difference_series_(y; s=12)\nprintln(\"Applied d=\", result.d, \", D=\", result.D)\n\n# Manual differencing specification\nresult = difference_series_(y; d=1, D=1, s=12)\ndifferenced_series = result.y_diff\n\n# Non-seasonal differencing only\nresult = difference_series_(y; d=1, s=nothing)\n\nMathematical Background\n\nNon-seasonal differencing: ∇ᵈyₜ = (1-B)ᵈyₜ\nSeasonal differencing: ∇ˢᴰyₜ = (1-Bˢ)ᴰyₜ\nCombined: ∇ᵈ∇ˢᴰyₜ = (1-B)ᵈ(1-Bˢ)ᴰyₜ\n\nReferences\n\nKwiatkowski, D., Phillips, P.C.B., Schmidt, P., & Shin, Y. (1992). \"Testing the null hypothesis of stationarity against the alternative of a unit root\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.dm_test-Tuple{Vector{<:Real}, Vector{<:Real}, Vector{<:Real}}","page":"API Reference","title":"Pythia.dm_test","text":"dm_test(actual, pred1, pred2; h=1, crit=\"MSE\", power=2)\n\nPerforms the Diebold-Mariano (DM) test to compare forecast accuracy between two prediction methods.\n\nArguments\n\nactual::Vector{<:Real}: The actual observed values.\npred1::Vector{<:Real}: The predicted values from model 1.\npred2::Vector{<:Real}: The predicted values from model 2.\n\nKeyword Arguments\n\nh::Int: The forecast horizon (must be > 0 and < length of the input vectors).\ncrit::String: Criterion to evaluate forecast errors. One of \"MSE\", \"MAD\", \"MAPE\", or \"poly\".\npower::Real: Used only when crit=\"poly\", exponent to apply to the error.\n\nReturns\n\nA NamedTuple: (DM = test_statistic, p_value = p_value)\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.fit-Tuple{ARIMAModel}","page":"API Reference","title":"Pythia.fit","text":"fit(model::ARIMAModel) -> ARIMAModel\n\nFit ARIMA model using the Hyndman-Khandakar automatic selection algorithm.\n\nThis is the main fitting function that implements the complete automatic ARIMA  algorithm, including differencing, parameter search, and model selection based  on information criteria.\n\nArguments\n\nmodel::ARIMAModel: ARIMA model configuration\n\nReturns\n\nARIMAModel: Fitted model with optimal parameters\n\nExamples\n\n# Automatic ARIMA for monthly data\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nmodel = ARIMAModel(y; s=12)\nfitted_model = fit(model)\n\nprintln(\"Selected model: ARIMA(\", fitted_model.p, \",\", fitted_model.d, \",\", fitted_model.q, \")\")\nprintln(\"Seasonal: (\", fitted_model.P, \",\", fitted_model.D, \",\", fitted_model.Q, \")[\", fitted_model.s, \"]\")\nprintln(\"AICc: \", fitted_model.best_ic)\n\n# Manual specification with fitting\nmodel = ARIMAModel(y; p=1, d=1, q=1, auto=false)\nfitted_model = fit(model)\n\n# Custom settings for high-frequency data\ndaily_data = randn(365) .+ 0.1 * sin.(2π * (1:365) / 365)\nmodel = ARIMAModel(daily_data; \n                   s=7,              # Weekly seasonality\n                   stepwise=false,   # Full grid search\n                   approximation=false,  # Exact ML\n                   trace=true)       # Show progress\nfitted_model = fit(model)\n\nAlgorithm Steps (Hyndman-Khandakar)\n\nDetermine differencing orders (d, D) using unit root tests\nSelect starting models from a restricted set\nUse stepwise search around the best starting model\nSelect final model based on information criterion (AICc)\n\nReferences\n\nHyndman, R.J., & Khandakar, Y. (2008). \"Automatic time series forecasting: the forecast package for R\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.fit-Tuple{SES}","page":"API Reference","title":"Pythia.fit","text":"fit!(model::ETSModel) Description:     fits an ETSModel object     - computes initial values of parameters: 'alpha' and 'initvalue'     - modifies the 'model' directly.     - after fit! is called, predict(model) is ready to be called Parameters:     - model: SES struct     - v: verbosity, default = 0. if v > 0, verbose will be used. Computation of model parameters: 'alpha' and 'initvalue' are computed using LBFGS Returns:     - nothing\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.fit_arima_manual_-Tuple{ARIMAModel}","page":"API Reference","title":"Pythia.fit_arima_manual_","text":"fit_arima_manual_(model::ARIMAModel) -> ARIMAModel\n\nFit ARIMA model parameters using nonlinear optimization.\n\nThis function estimates ARIMA parameters by minimizing the specified loss function  using the configured optimization algorithm. It handles parameter constraints and  computes model statistics.\n\nArguments\n\nmodel::ARIMAModel: ARIMA model with specified orders and configuration\n\nReturns\n\nARIMAModel: The same model with fitted parameters stored in fitted_model\n\nExamples\n\n# Fit a specific ARIMA(1,1,1) model\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nmodel = ARIMAModel(y; p=1, d=1, q=1, auto=false)\nfitted_model = fit_arima_manual_(model)\n\n# Access fitted parameters\nparams = fitted_model.fitted_model.params\nprintln(\"Fitted parameters: \", params)\nprintln(\"Log-likelihood: \", fitted_model.fitted_model.loglik)\nprintln(\"Residual variance: \", fitted_model.fitted_model.sigma2)\n\n# Fit with custom optimizer\nmodel = ARIMAModel(y; p=2, d=1, q=1, \n                   optimizer=() -> Optim.NelderMead(),\n                   auto=false)\nfitted_model = fit_arima_manual_(model)\n\nMathematical Details\n\nThe function optimizes the parameter vector θ = [μ, φ₁, ..., φₚ, θ₁, ..., θₑ, Φ₁, ..., Φₚ, Θ₁, ..., Θₖ] to minimize the loss function L(θ).\n\nThe intercept μ is included only when (d + D) < 2.\n\nReferences\n\nNocedal, J., & Wright, S. (2006). \"Numerical Optimization\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.get_ic-Tuple{ARIMAModel}","page":"API Reference","title":"Pythia.get_ic","text":"get_ic(model::ARIMAModel) -> Float64\n\nCalculate the corrected Akaike Information Criterion (AICc) for a fitted ARIMA model.\n\nThe AICc is particularly useful for small samples and is defined as: AICc = AIC + (2k(k+1))/(n-k-1)\n\nWhere:\n\nAIC = -2 * log-likelihood + 2k\nk = number of parameters\nn = sample size\n\nArguments\n\nmodel::ARIMAModel: A fitted ARIMA model\n\nReturns\n\nFloat64: The AICc value (lower is better)\n\nExamples\n\n# Fit model and get AICc\nmodel = ARIMAModel(y; p=1, d=1, q=1)\nfitted_model = fit(model)\naicc_value = get_ic(fitted_model)\nprintln(\"AICc: \", aicc_value)\n\nMathematical Formula\n\nAICc = -2ℓ + 2k + (2k(k+1))/(n-k-1)\n\nWhere ℓ is the maximized log-likelihood.\n\nReferences\n\nHurvich, C.M., & Tsai, C.L. (1989). \"Regression and time series model selection in small samples\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.inverse_difference-Tuple{Vector{Float64}, Float64, Int64}","page":"API Reference","title":"Pythia.inverse_difference","text":"inverse_difference(forecasts, y_last, d) -> Vector{Float64}\n\nReverse non-seasonal differencing transformation to convert forecasts back to original scale.\n\nThis function undoes the non-seasonal differencing operation (1-B)ᵈ by iteratively  computing cumulative sums.\n\nArguments\n\nforecasts::Vector{Float64}: Forecasts in the differenced scale\ny_last::Float64: Last observation from the original series\nd::Int: Non-seasonal differencing order\n\nReturns\n\nVector{Float64}: Forecasts transformed back to original scale\n\nExamples\n\n# First difference (d=1)\nforecasts = [0.5, 0.3, -0.2, 0.8]  # Forecasts in first-differenced scale\ny_last = 120.0  # Last observation from original series\nd = 1\n\n# Transform back to original scale\noriginal_forecasts = inverse_difference(forecasts, y_last, d)\nprintln(\"Original scale forecasts: \", original_forecasts)\n# Output: [120.5, 120.8, 120.6, 121.4]\n\n# Second difference (d=2)\nforecasts = [0.1, 0.05, -0.02, 0.1]\ny_last = 100.0\nd = 2\n\noriginal_forecasts = inverse_difference(forecasts, y_last, d)\n\nMathematical Formula\n\nFor d = 1: yₜ = ∇yₜ + yₜ₋₁ = yₜ₋₁ + Σᵢ₌₁ᵗ ∇yᵢ For d = 2: yₜ = ∇²yₜ + 2yₜ₋₁ - yₜ₋₂\n\nWhere ∇ = (1-B) is the difference operator.\n\nReferences\n\nHamilton, J.D. (1994). \"Time Series Analysis\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.inverse_seasonal_difference-Tuple{Vector{Float64}, Vector{Float64}, Int64, Int64}","page":"API Reference","title":"Pythia.inverse_seasonal_difference","text":"inverse_seasonal_difference(forecasts, y_last, s, D) -> Vector{Float64}\n\nReverse seasonal differencing transformation to convert forecasts back to original scale.\n\nThis function undoes the seasonal differencing operation (1-Bˢ)ᴰ by iteratively  adding back the seasonal lags.\n\nArguments\n\nforecasts::Vector{Float64}: Forecasts in the seasonally differenced scale\ny_last::Vector{Float64}: Last s×D observations from the original series\ns::Int: Seasonal period\nD::Int: Seasonal differencing order\n\nReturns\n\nVector{Float64}: Forecasts transformed back to original scale\n\nExamples\n\n# Monthly data with one seasonal difference\nforecasts = [0.1, 0.2, -0.1, 0.3]  # Forecasts in differenced scale\ny_last = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]  # Last 12 months\ns = 12  # Monthly seasonality\nD = 1   # One seasonal difference\n\n# Transform back to original scale\noriginal_forecasts = inverse_seasonal_difference(forecasts, y_last, s, D)\nprintln(\"Original scale forecasts: \", original_forecasts)\n\n# Quarterly data with two seasonal differences\nforecasts = [0.05, 0.1, -0.02, 0.08]\ny_last = [100, 102, 98, 105, 108, 110, 106, 112]  # Last 8 quarters\ns = 4\nD = 2\n\noriginal_forecasts = inverse_seasonal_difference(forecasts, y_last, s, D)\n\nMathematical Formula\n\nFor D = 1: yₜ = ∇ₛyₜ + yₜ₋ₛ For D = 2: yₜ = ∇ₛ²yₜ + 2yₜ₋ₛ - yₜ₋₂ₛ\n\nWhere ∇ₛ = (1-Bˢ) is the seasonal difference operator.\n\nReferences\n\nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (2015). \"Time Series Analysis: Forecasting and Control\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.kpss_test-Tuple{AbstractVector{<:Real}}","page":"API Reference","title":"Pythia.kpss_test","text":"kpss_test(y::AbstractVector{<:Real};\n          regression::String = \"c\",\n          nlags::Union{String, Int} = \"auto\")\n\nPerform the KPSS test for level or trend stationarity.\n\nArguments\n\ny: Time series data (1D vector).\nregression: \"c\" for constant, \"ct\" for trend.\nnlags: \"auto\" (default, Hobijn-style), \"legacy\" (Schwert), or an integer.\n\nReturns\n\nNamedTuple with:\n\nstatistic: KPSS test statistic\npvalue: Approximate p-value\nlags: Lag used\ncritical_values: Dict of critical values at 10%, 5%, 1%\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.makeForecast_-Tuple{SES}","page":"API Reference","title":"Pythia.makeForecast_","text":"makeForecast() Description:     - Helper function     - computes a forecast with Simple Exponential Smoothing (SES) [1] Parameters     - model: SES struct (defined above)     - Note: when makeForecast is called, all parameters have been calibrated. Returns:     A tuple containing the following:     - Vector{Float64} forecast: contains fitted values     - SSE::Float64 SSE: the Sum of the Squares (error term)         - used for parameter optimzation References:      [1] Hyndman, R.J., & Athanasopoulos, G. (2019) Forecasting:         principles and practice, 3rd edition, OTexts: Melbourne,         Australia. OTexts.com/fpp3. Accessed on April 19th 2020.\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.plot_acf_pacf-Tuple{AbstractVector{<:Real}}","page":"API Reference","title":"Pythia.plot_acf_pacf","text":"plot_acf_pacf(y; lags=20, alpha=0.05) -> Plot\n\nCompute and plot the autocorrelation function (ACF) and partial autocorrelation function (PACF)  for a univariate time series, including approximate confidence bounds.\n\nArguments\n\ny::AbstractVector{<:Real}: Time series data\n\nKeyword Arguments\n\nlags::Int=20: Number of lags to compute and display for ACF and PACF\nalpha::Float64=0.05: Significance level for confidence bounds (default 95% confidence)\n\nReturns\n\nPlot: Combined plot with ACF on top and PACF below, including confidence bounds\n\nExamples\n\nusing Random\nRandom.seed!(123)\ny = randn(100)\n\n# Default 20 lags with 95% confidence bounds\nplot_acf_pacf(y)\n\n# Custom number of lags\nplot_acf_pacf(y, lags=30, alpha=0.01)\n\nNotes\n\nACF includes lag 0, while PACF typically starts from lag 1.\nThe plots can help detect:\nAutoregressive (AR) structure: PACF cuts off after lag p\nMoving average (MA) structure: ACF cuts off after lag q\n\nReferences\n\nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (2015). Time Series Analysis: Forecasting and Control.\nBrockwell, P.J., & Davis, R.A. (2016). Introduction to Time Series and Forecasting.\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.plot_vectors-Tuple{Vector{<:AbstractVector}, Vector{String}, Vector{Symbol}}","page":"API Reference","title":"Pythia.plot_vectors","text":"plot_vectors(vectors::Vector{<:AbstractVector}, \n             labels::Vector{String}, \n             colors::Vector{Symbol}; \n             linestyles::Union{Nothing, Vector{Symbol}} = nothing)\n\nPlot multiple vectors on the same plot. \n\nArguments:\n\nvectors: A list of vectors to plot.\nlabels: Labels for each line (used in the legend).\ncolors: Line colors for each vector.\n\nOptional keyword:\n\nlinestyles: Optional line styles (e.g., :solid, :dash, :dot, etc.)\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.predict-Tuple{ARIMAModel}","page":"API Reference","title":"Pythia.predict","text":"predict(model::ARIMAModel; h=model.h,level) -> NamedTuple{(:fittedvalues, :lower, :upper), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}\n\nGenerate forecasts from a fitted ARIMA model.\n\nThis function produces point forecasts by iteratively applying the ARIMA equation  and then transforming back to the original scale by reversing any differencing operations.\n\nArguments\n\nmodel::ARIMAModel: A fitted ARIMA model\n\nKeyword Arguments\n\nh::Int=model.h: Forecast horizon (number of periods to forecast)\nlevel::Float64 = 0.95: Confidence level for the forecast intervals.\n\nReturns\n\nNamedTuple with:\nfittedvalues::Vector{Float64}: Forecasted values on the original scale.\nlower::Vector{Float64}: Lower bounds of the prediction intervals.\nupper::Vector{Float64}: Upper bounds of the prediction intervals.\n\nExamples\n\n# Basic forecasting\ny = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120]\nmodel = ARIMAModel(y; s=12, h=6)\nfitted_model = fit(model)\nresults = predict(fitted_model)\nforecasts = results.fittedvalues\nlower = results.lower\nupper = results.upper\n\n# Display original data and forecasts\nn = length(y)\nprintln(\"Historical data: \", y)\nprintln(\"Forecasts: \", forecasts)\n\n# Custom forecast horizon\nlong_forecasts = predict(fitted_model; h=12)  # 12-period forecast\n\n# Seasonal forecasting example\nmonthly_data = [100, 102, 98, 105, 108, 110, 106, 112, 115, 118, 114, 120,\n                125, 128, 122, 132, 135, 138, 130, 142, 145, 148, 140, 155]\nmodel = ARIMAModel(monthly_data; s=12, h=6)\nfitted_model = fit(model)\nresults = predict(fitted_model)\n\n# Extract just the forecast values\nforecast_values = results.fittedvalues\nprintln(\"6-month ahead forecasts: \", forecast_values)\n\nMathematical Details\n\nFor ARIMA(p,d,q)×(P,D,Q)ₓ, the h-step ahead forecast is: ŷₙ₊ₕ = μ + Σᵢ₌₁ᵖ φᵢŷₙ₊ₕ₋ᵢ + Σᵢ₌₁ᵖ Φᵢŷₙ₊ₓ₋ᵢₛ + Σⱼ₌₁ᵈ θⱼeₙ₊ₕ₋ⱼ + Σⱼ₌₁ᵈ Θⱼeₙ₊ₕ₋ⱼₛ\n\nWhere:\n\nFuture errors eₙ₊ₖ = 0 for k > 0\nPast values and errors are used when available\nForecasts are used for future values in AR terms\n\nReferences\n\nHyndman, R.J., & Athanasopoulos, G. (2021). \"Forecasting: Principles and Practice\"\nBox, G.E.P., Jenkins, G.M., & Reinsel, G.C. (2015). \"Time Series Analysis: Forecasting and Control\"\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.predict-Tuple{Pythia.ETSModel}","page":"API Reference","title":"Pythia.predict","text":"predict() Description:     - Exposed function     - computes a forecast with Simple Exponential Smoothing (SES) [1]     - calls makeForecast_ a helper function, which returns a vector of fitted values and SSE error term. Parameters     - model: SES struct (defined above)     - Note: when predict is called, all parameters have been calibrated. Returns:     A tuple containing the following:     - Vector{Float64} forecast: contains fitted values References:      [1] Hyndman, R.J., & Athanasopoulos, G. (2019) Forecasting:         principles and practice, 3rd edition, OTexts: Melbourne,         Australia. OTexts.com/fpp3. Accessed on April 19th 2020.\n\n\n\n\n\n","category":"method"},{"location":"api/#Pythia.sse_arima_loss_-Tuple{AbstractVector{<:Real}, Vector{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64, Int64}","page":"API Reference","title":"Pythia.sse_arima_loss_","text":"sse_arima_loss_(y, φ, θq, Φ, Θ, μ, s; return_residuals=false) -> Float64 or Vector{Float64}\n\nCalculate the Sum of Squared Errors (SSE) loss for ARIMA model parameters.\n\nThis function provides exact maximum likelihood estimation by computing the full  likelihood function. It's more accurate than CSS but computationally more intensive.\n\nArguments\n\ny::AbstractVector{<:Real}: Time series data\nφ::Vector{Float64}: Non-seasonal AR coefficients [φ₁, φ₂, ..., φₚ]\nθq::Vector{Float64}: Non-seasonal MA coefficients [θ₁, θ₂, ..., θₑ]\nΦ::Vector{Float64}: Seasonal AR coefficients [Φ₁, Φ₂, ..., Φₚ]\nΘ::Vector{Float64}: Seasonal MA coefficients [Θ₁, Θ₂, ..., Θₖ]\nμ::Float64: Intercept/mean parameter\ns::Int: Seasonal period\n\nKeyword Arguments\n\nreturn_residuals::Bool=false: Return residuals instead of sum of squares\n\nReturns\n\nFloat64: Sum of squared residuals (if return_residuals=false)\nVector{Float64}: Residual series (if return_residuals=true)\n\nExamples\n\n# Example for ARIMA(2,1,1) model\ny = cumsum(randn(100)) + 0.1*(1:100)  # Random walk with drift\nφ = [0.3, -0.2]   # AR(2) coefficients\nθq = [0.4]        # MA(1) coefficient\nΦ = Float64[]     # No seasonal AR\nΘ = Float64[]     # No seasonal MA\nμ = 0.1           # Small drift\ns = 1             # No seasonality\n\n# Calculate exact loss\nloss = sse_arima_loss_(y, φ, θq, Φ, Θ, μ, s)\nprintln(\"SSE Loss: \", loss)\n\n# Get residuals for diagnostic checking\nresiduals = sse_arima_loss_(y, φ, θq, Φ, Θ, μ, s; return_residuals=true)\n\nMathematical Formula\n\nThe exact likelihood residuals are computed as: eₜ = yₜ - μ - Σᵢ₌₁ᵖ φᵢyₜ₋ᵢ - Σᵢ₌₁ᵖ Φᵢyₜ₋ᵢₛ - Σⱼ₌₁ᵈ θⱼeₜ₋ⱼ - Σⱼ₌₁ᵈ Θⱼeₜ₋ⱼₛ\n\nFor t = 1, 2, ..., n (handling initial conditions appropriately).\n\nReferences\n\nHamilton, J.D. (1994). \"Time Series Analysis\"\nBrockwell, P.J., & Davis, R.A. (2016). \"Introduction to Time Series and Forecasting\"\n\n\n\n\n\n","category":"method"},{"location":"examples/arma_generated/#Autoregressive-Moving-Average-(ARMA):-Artificial-data","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"In this example, we will:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"Generate artificial ARMA(1,1) data.  \nSplit the data into training and test sets.  \nFit an ARIMA(1,0,1) model (equivalent to ARMA(1,1)).  \nForecast the next 5 values with 95% prediction intervals.","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"","category":"page"},{"location":"examples/arma_generated/#Step-1.-Simulate-ARMA(1,1)-data","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Step 1. Simulate ARMA(1,1) data","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"We define a helper function to generate an ARMA(1,1) series:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"using Random, Pythia\n\nfunction simulate_arma(rng::AbstractRNG, ϕ::Float64, θ::Float64, σ::Float64, n::Int)\n    ε = randn(rng, n) .* σ\n    y = zeros(n)\n    y[1] = 5\n    for t in 2:n\n        y[t] = ϕ * y[t-1] + ε[t] + θ * ε[t-1]\n    end\n    return y\nend","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"Now generate 2000 data points with parameters ϕ = 0.6, θ = 0.3, and σ = 1:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"using Random\nrng = Random.MersenneTwister(40)\nϕ, θ, σ = 0.6, 0.3, 1.0\n\ny = simulate_arma(rng, ϕ, θ, σ, 2000)\n\nlength(y)  # confirm we generated 2000 values","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"","category":"page"},{"location":"examples/arma_generated/#Step-2.-Train-test-split","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Step 2. Train-test split","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"We’ll keep the first 1995 points for training, and the last 5 points for testing:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"y_train = y[1:1995]\ny_test  = y[1996:2000]\n\n(size(y_train), size(y_test))","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"","category":"page"},{"location":"examples/arma_generated/#Step-3.-Fit-ARIMA(1,0,1)-model","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Step 3. Fit ARIMA(1,0,1) model","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"An ARMA(1,1) is equivalent to ARIMA with order (p=1, d=0, q=1).   We fit the model on the training data:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"using Pythia\nmodel = Pythia.ARIMAModel(y_train; p=1, q=1, P=0, Q=0, seasonal=false, stationary=true, approximation=false)\nfitted_model = fit(model)","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"","category":"page"},{"location":"examples/arma_generated/#Step-4.-Forecast-next-5-values","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Step 4. Forecast next 5 values","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"Finally, we forecast the next 5 points, with 95% prediction intervals:","category":"page"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"res   = predict(fitted_model, h=5, level=0.95)\npreds = res.fittedvalues\nlower = res.lower\nupper = res.upper","category":"page"},{"location":"examples/arma_generated/#Forecast-Plot-(Last-50-points)","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Forecast Plot (Last 50 points)","text":"","category":"section"},{"location":"examples/arma_generated/","page":"Autoregressive Moving Average (ARMA): Artificial data","title":"Autoregressive Moving Average (ARMA): Artificial data","text":"using Plots\n\nwindow = 50\ny_last = y_train[end-window+1:end]\nx_last = (length(y_train)-window+1):length(y_train)\n\nh = length(preds)\nx_forecast = length(y_train)+1 : length(y_train)+h\n\n# Plot last 50 points of y_train\nplt = plot(x_last, y_last, label=\"y_train (last 50)\", color=:blue)\n\n# Plot forecast\nplot!(plt, x_forecast, preds, label=\"Forecast\", color=:red, lw=2)\n\n# Plot upper and lower as light red lines\nplot!(plt, x_forecast, lower, label=\"Lower 95%\", color=:red, alpha=0.3, lw=2, linestyle=:dash)\nplot!(plt, x_forecast, upper, label=\"Upper 95%\", color=:red, alpha=0.3, lw=2, linestyle=:dash)\n\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Forecast vs Observed (last 50 points)\")\nplt","category":"page"},{"location":"examples/stationarity_sun/#Stationarity-and-detrending-(ADF/KPSS)","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"","category":"section"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Stationarity means that the statistical properties of a time series i.e. mean, variance and covariance do not change over time. Many statistical models require the series to be stationary to make effective and precise predictions.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Two statistical tests would be used to check the stationarity of a time series – Augmented Dickey Fuller (“ADF”) test and Kwiatkowski-Phillips-Schmidt-Shin (“KPSS”) test. A method to convert a non-stationary time series into stationary series shall also be used.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"This first cell imports standard packages and sets plots to appear inline.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"using Pythia,Plots","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Sunspots dataset is used. It contains yearly (1700-2008) data on sunspots from the National Geophysical Data Center.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"sun_data = load_dataset(\"sunspots\")\nyears = round.(Int, sun_data.Year)\nssn   = sun_data.SSN","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"The data is plotted now.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"plot(\n    years,\n    ssn,\n    xlabel = \"Year\",\n    ylabel = \"Sunspot Number (SSN)\",\n    title = \"Yearly Sunspot Activity\",\n    lw = 2,\n    legend = false,\n    size = (1200, 800)\n)","category":"page"},{"location":"examples/stationarity_sun/#Stationarity-tests","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity tests","text":"","category":"section"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"KPSS is a test for checking the stationarity of a time series.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Null Hypothesis: The process is trend stationary.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Alternate Hypothesis: The series has a unit root (series is not stationary).","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"kpss_test(ssn)","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Based upon the significance level of 0.05 and the p-value of KPSS test, there is evidence for rejecting the null hypothesis in favor of the alternative. Hence, the series is non-stationary as per the KPSS test.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"ADF test is used to determine the presence of unit root in the series, and hence helps in understand if the series is stationary or not. The null and alternate hypothesis of this test are:","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Null Hypothesis: The series has a unit root.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Alternate Hypothesis: The series has no unit root.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"If the null hypothesis in failed to be rejected, this test may provide evidence that the series is non-stationary.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"adf_test(ssn)","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Based upon the significance level of 0.05 and the p-value of ADF test, the null hypothesis can not be rejected. Hence, the series is non-stationary.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"It is always better to apply both the tests, so that it can be ensured that the series is truly stationary. Possible outcomes of applying these stationary tests are as follows:","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Case 1: Both tests conclude that the series is not stationary - The series is not stationary Case 2: Both tests conclude that the series is stationary - The series is stationary Case 3: KPSS indicates stationarity and ADF indicates non-stationarity - The series is trend stationary. Trend needs to be removed to make series strict stationary. The detrended series is checked for stationarity. Case 4: KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary. Differencing is to be used to make series stationary. The differenced series is checked for stationarity. Here, due to the difference in the results from ADF test and KPSS test, it can be inferred that the series is trend stationary and not strict stationary. The series can be detrended by differencing or by model fitting.","category":"page"},{"location":"examples/stationarity_sun/#Detrending-by-Differencing","page":"Stationarity and detrending (ADF/KPSS)","title":"Detrending by Differencing","text":"","category":"section"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"It is one of the simplest methods for detrending a time series. A new series is constructed where the value at the current time step is calculated as the difference between the original observation and the observation at the previous time step.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Differencing is applied on the data and the result is plotted.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"y = difference(ssn)\nssn_diff = y.series","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"This applies auto differencing on the data till it passes the KPSS test","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"years = years[1:end-1]\nplot(\n    years,\n    ssn_diff,\n    xlabel = \"Year\",\n    ylabel = \"Sunspot Number (SSN)\",\n    title = \"Yearly Sunspot Activity\",\n    lw = 2,\n    legend = false,\n    size = (1200, 800)\n)","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"ADF test is now applied on these detrended values and stationarity is checked.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"adf_test(ssn_diff)","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Based upon the p-value of ADF test, the series is strict stationary now.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"KPSS test is now applied on these detrended values and stationarity is checked.","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"kpss_test(ssn_diff)","category":"page"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Based upon the p-value of KPSS test, the null hypothesis can not be rejected. Hence, the series is stationary.","category":"page"},{"location":"examples/stationarity_sun/#Conclusion","page":"Stationarity and detrending (ADF/KPSS)","title":"Conclusion","text":"","category":"section"},{"location":"examples/stationarity_sun/","page":"Stationarity and detrending (ADF/KPSS)","title":"Stationarity and detrending (ADF/KPSS)","text":"Two tests for checking the stationarity of a time series are used, namely ADF test and KPSS test. Detrending is carried out by using differencing. Trend stationary time series is converted into strict stationary time series. Requisite forecasting model can now be applied on a stationary time series data.","category":"page"},{"location":"#Pythia.jl","page":"Home","title":"Pythia.jl","text":"","category":"section"}]
}
